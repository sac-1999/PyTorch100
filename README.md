ðŸ§  100 PyTorch Practice Tasks
ðŸ”¹ 1. Fundamentals & Tensor Operations (10)
Implement matrix multiplication manually using PyTorch tensors (no torch.mm).

Write a function to normalize tensors (min-max scaling).

Create a custom tensor broadcasting example.

Implement element-wise operations without built-in functions.

Compute gradients manually using autograd.

Write a script to compute cosine similarity between two tensors.

Implement tensor reshaping and flattening operations.

Build a one-hot encoding function using tensors.

Implement tensor slicing for 3D tensors.

Write a script to measure tensor operation speed on CPU vs GPU.

ðŸ”¹ 2. Custom Autograd & Backpropagation (10)
Build a custom autograd function for ReLU.

Implement backpropagation for linear regression without nn.Module.

Write a custom loss function (Huber loss).

Create a custom optimizer (SGD with momentum).

Implement gradient clipping manually.

Build a custom autograd function for softmax.

Implement cross-entropy loss manually.

Write a script to visualize gradient flow in a network.

Implement weight decay manually.

Compare training with and without gradient accumulation.

ðŸ”¹ 3. Neural Network Basics (10)
Build a feedforward neural network from scratch using nn.Module.

Train a network on MNIST digits without torchvision.

Implement dropout manually.

Compare training with and without batch normalization.

Visualize weight updates during training.

Implement early stopping manually.

Build a multilayer perceptron for tabular data.

Train a model with different activation functions (ReLU, Tanh, Sigmoid).

Implement learning rate scheduling manually.

Write a script to log training metrics with TensorBoard.

ðŸ”¹ 4. CNNs (Computer Vision) (10)
Implement a CNN for CIFAR-10 classification.

Write a custom convolution layer (no nn.Conv2d).

Build a ResNet-like skip connection manually.

Train a CNN with data augmentation.

Implement Grad-CAM visualization.

Build a CNN for fashion-MNIST dataset.

Implement depthwise separable convolution.

Compare CNN performance with and without pooling.

Implement dilated convolution manually.

Train a CNN with transfer learning from pretrained models.

ðŸ”¹ 5. RNNs & Sequence Models (10)
Build a character-level RNN for text generation.

Implement LSTM manually (no nn.LSTM).

Train a sentiment analysis model on IMDB dataset.

Build a sequence-to-sequence model for translation.

Implement attention mechanism for RNNs.

Train a model for next-word prediction.

Implement GRU manually.

Build a time-series forecasting model.

Train a model for named entity recognition.

Implement beam search for sequence generation.

ðŸ”¹ 6. Transformers (10)
Implement scaled dot-product attention from scratch.

Build a mini Transformer encoder for text classification.

Train a Transformer on news headlines.

Implement positional encoding manually.

Compare Transformer vs LSTM on sequence tasks.

Build a Transformer decoder for text generation.

Implement multi-head attention manually.

Train a Transformer for machine translation.

Implement masked language modeling (like BERT).

Build a Vision Transformer (ViT) for CIFAR-100.

ðŸ”¹ 7. GANs (Generative Models) (10)
Implement a vanilla GAN for MNIST.

Build a DCGAN for CIFAR-10.

Train a conditional GAN (cGAN).

Implement Wasserstein GAN with gradient penalty.

Build a CycleGAN for image-to-image translation.

Implement Pix2Pix GAN.

Train a GAN for super-resolution.

Build a StyleGAN-like architecture.

Implement progressive growing GAN.

Compare GAN vs VAE for image generation.

ðŸ”¹ 8. Advanced Topics (10)
Implement mixed precision training with torch.cuda.amp.

Train a model with distributed data parallel (DDP).

Write a script for model quantization.

Implement pruning on a CNN.

Build a reinforcement learning agent with policy gradients.

Implement actor-critic reinforcement learning.

Train a model with curriculum learning.

Implement knowledge distillation between models.

Build a meta-learning model (MAML).

Implement contrastive learning (SimCLR).

ðŸ”¹ 9. Deployment & Optimization (10)
Export a PyTorch model to ONNX format.

Optimize inference with TorchScript.

Deploy a PyTorch model with FastAPI.

Implement model checkpointing and resuming training.

Write a script to monitor GPU usage during training.

Build a REST API for serving PyTorch models.

Deploy a model with Flask.

Implement quantization-aware training.

Optimize inference with TensorRT.

Deploy a PyTorch model on mobile.

ðŸ”¹ 10. Research-Level Challenges (10)
Train a Vision Transformer (ViT) on CIFAR-100.

Build a BERT-like model for masked language modeling.

Implement diffusion models for image generation.

Reproduce ResNet results using PyTorch.

Reproduce GPT-2 results using PyTorch.

Implement reinforcement learning with PPO.

Train a graph neural network (GNN).

Implement Deep Q-Learning.

Build a neural style transfer model.

Implement self-supervised learning with BYOL.

âœ… Summary
Thatâ€™s 100 PyTorch practice tasks, covering:

Fundamentals

Autograd

Neural networks

CNNs

RNNs

Transformers

GANs

Advanced topics

Deployment

Research-level challenges
